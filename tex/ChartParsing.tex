\chapter{The Importance of Data Structures}
\label{cha:ChartParsing}

Remember that we analyze parsers as the combination of three components: a parsing schema, a control structure, and a data structure.
While parsing schema and certain aspects of the control structure have occupied a lot of our attention, we haven't talked much about data structures so far.
Our psycholinguistic evaluations used a prefix tree as a representation of the parse space and how it might be explored by the parser, but we did not assume that the parser actually uses a prefix tree to store information.
However, we did propose that the control structure might operate on a priority queue as an encoding of which parse to explore next.
A priority queue is a form of data structure as it holds the parse items inferred by the parser.
But it cannot be the full data structure: since items are frequently removed from the queue, it does not provide a permanent record of the actual parse.
One option would be to build and store each individual parse trees in parallel with the parse, but as we will see this is too inefficient even for computers, whose working memory vastly outstrips that of humans.
It is about time, then, that we take a more careful look at data structures and the parsers that make use of them. 

\section{Why Data Structures Matter}

Recall from Chap.~\ref{cha:BigPicture} that we distinguish between parsers and recognizers.
A recognizer only has to determine for a given sentence whether it is well-formed, whereas a parser also has to find the right tree structures.

One might think that data structures are less of a concern for recognizers.
Any one of our parsing schema coupled with a priority queue yields a working recognizer.
The fact that the priority queue does not store parse items after they have been used in an inference rule is irrelevant since the only thing that matters is whether the priority queue contains a parse item that is a goal for the input sentence.
As soon as this condition is satisfied, the recognizer can stop and declare the input sentence well-formed.

Even for a recognizer, though, there are downsides to not storing previous items.
Most problematically, the recognizer may accidentally enter a loop if there are two items $I$ and $J$ such that each one can be inferred from the other.
If neither $I$ nor $J$ allow the sentence to be recognized, and the control structure is designed in such a way that $I$ and $J$ are preferred to other items that would eventually lead to successful recognition of the input, then the recognizer will try the $I$-route and the $J$-route over and over again.
In less severe cases the recognizer does not enter a loop but ends up exploring partial parses that have already turned out to be failures at an earlier point.
%
\begin{examplebox}[Needlees Reexploration of Failed Partial Parses]
    The parse history of the recursive descent parser\slash recognizer for the garden path sentence \emph{the horse raced past the barn} includes several cases of redundancy.
    %
    \begin{center}
        \footnotesize
        \begin{tikzpicture}[%
            level 1+/.style = { level distance = 2em },
            level 5/.style = { sibling distance= -5em },
            level 7/.style = { sibling distance= -5em },
            level 8/.style = { sibling distance= -10em }
            ]
            \Tree
                [.{[0,\psep S]}
                    [.{[0,\psep NP VP]}
                        [.{[0,\psep Det N VP]}
                            [.{[0, \psep the N VP]}
                                [.{[1,\psep N VP]}
                                    [.{[1,\psep horse VP]}
                                        [.{[2,\psep VP]}
                                            [.{[2,\psep V PP]}
                                                [.{[2,\psep raced PP]}
                                                    [.{[3,\psep PP]}
                                                        [.{[3,\psep P NP]}
                                                            [.{[3,\psep past NP]}
                                                                [.{[4,\psep NP]}
                                                                    [.{[4,\psep Det N]}
                                                                        [.{[4,\psep the N]}
                                                                            [.{[5,\psep N]}
                                                                                [.{[5,\psep barn]}
                                                                                    {[6,\psep]}
                                                                                ]
                                                                                {[5,\psep horse]}
                                                                            ]
                                                                        ]
                                                                    ]
                                                                    [.{[4,\psep Det N VP$_\mathit{rel}$]}
                                                                        [.{[4,\psep the N VP$_\mathit{rel}$]}
                                                                            [.{[5,\psep N VP$_\mathit{rel}$]}
                                                                                [.{[5,\psep barn VP$_\mathit{rel}$]}
                                                                                    [.{[6,\psep VP$_\mathit{rel}$]}
                                                                                        [.{[6,\psep V$_\mathit{rel}$ PP]}
                                                                                            {[6,\psep fell PP]}
                                                                                        ]
                                                                                    ]
                                                                                ]
                                                                                {[5,\psep horse VP$_\mathit{rel}$]}
                                                                            ]
                                                                        ]
                                                                    ]
                                                                ]
                                                            ]
                                                        ]
                                                    ]
                                                ]
                                                {[2,\psep fell PP]}
                                            ]
                                            [.{[2,\psep V]}
                                                {[2,\psep raced]}
                                                {[2,\psep fell]}
                                            ]
                                        ]
                                    ]
                                {[1,\psep barn VP]}
                                ]
                            ]
                        ]
                        [.{[0,\psep Det N VP$_\mathit{rel}$ VP]}
                            [.{[0,\psep the N VP$_\mathit{rel}$ VP]}
                                [.{[1,\psep N VP$_\mathit{rel}$ VP]}
                                    {[1,\psep barn VP$_\mathit{rel}$ VP]}
                                    [.{[1,\psep horse VP$_\mathit{rel}$ VP]}
                                        [.{[2,\psep VP$_\mathit{rel}$ VP]}
                                            [.{[2,\psep V$_\mathit{rel}$ PP VP]}
                                                [.{[3,\psep raced PP VP]}
                                                    [.{[4,\psep PP VP]}
                                                        [.{[4,\psep P NP VP]}
                                                            [.{[4,\psep past NP VP]}
                                                                [.{[5,\psep NP VP]}
                                                                    [.{[5,\psep Det N VP]}
                                                                        [.{[5,\psep the N VP]}
                                                                            [.{[6,\psep N VP]$_{10}$}
                                                                                [.{[6,\psep barn VP]}
                                                                                    [.{[7,\psep VP]}
                                                                                        [.{[7,\psep V]$_{13}$}
                                                                                            [.{[7,\psep fell]}
                                                                                                {[7,\psep]}
                                                                                            ]
                                                                                        ]
                                                                                    ]
                                                                                ]
                                                                            ]
                                                                        ]
                                                                    ]
                                                                ]
                                                            ]
                                                        ]
                                                    ]
                                                ]
                                            ]
                                        ]
                                    ]
                                ]
                            ]
                        ]
                    ]
                ]
        \end{tikzpicture}
    \end{center}
    %
    After the failed exploration of the left-most branch, the recognizer backtracks and uses [5,\psep N] to infer [5,\psep horse] instead of [5,\psep barn].
    Since the word at position 5 is \emph{horse}, not \emph{barn}, this branch obviously fails, too.
    What more, we can infer that every  parse item of the form [5,\psep horse $\gamma$] will lead to failure.
    Yet the parser attempts the very same thing in the next branch with [5,\psep horse VP$_\mathit{rel}$].
    Later on, a similar instance of redundancy arises when [1, \psep barn VP$_\mathit{rel}$ VP] is conjectured even though [1,\psep barn VP] has already failed.
\end{examplebox}
%
\noindent
If parse items are discarded immediately after being used in an inference rule, the recognizer also loses the ability to recycle successful partial parses.
%
\begin{examplebox}[Needless Reexploration of Successful Partial Parses]
    Suppose the sentence \emph{Red Riding Hood met the big, bad wolf yesterday} is to be recognized using the following grammar:
    %
    \begin{center}
        \begin{tabular}{rcl@{\hspace{2em}}rcl}
            S    & \rewrite & NP VP
                 & 
            PN   & \rewrite & Red Riding Hood
            \\
            NP   & \rewrite & PN | Det N | Det AP N
                 & 
            Det  & \rewrite & the
            \\
            AP   & \rewrite & Adj | Adj AP
                 & 
            N    & \rewrite & wolf
            \\
            VP   & \rewrite & Vi | Vt NP | VP AdvP
                 & 
            Adj  & \rewrite & bad | big
            \\
            AdvP & \rewrite & Adv
                 & 
            Adv  & \rewrite & yesterday
            \\
                 &          &                      & 
            Vi   & \rewrite & left
            \\
                 &          &                      & 
            Vt   & \rewrite & left | met
        \end{tabular}
    \end{center}
    %
    Assume furthermore that the linear order of disjunctive right-hand sides encodes their priority in the control structure.
    Then a recursive descent recognizer will first go through the options below rewriting VP as Vi or Vt NP, neither of which produces a goal item.
    %
    \begin{center}
        \footnotesize
        \begin{tikzpicture}[
            level 1+/.style = { level distance = 2em }
            ]
            \Tree
                [.{[0,\psep S]}
                    [.{[0,\psep NP VP]}
                        [.{[0,\psep PN VP]}
                            [.{[0,\psep RRH VP]}
                                [.{[1,\psep VP]}
                                    [.{[1,\psep Vi]}
                                        [.{[1, \psep left]}
                                        ]
                                    ]
                                    [.{[1,\psep Vt NP]}
                                        [.{[1,\psep left NP]}
                                        ]
                                        [.{[1,\psep met NP]}
                                            [.{[2,\psep NP]}
                                                [.{[2,\psep PN]}
                                                    [.{[2,\psep RRH]}
                                                    ]
                                                ]
                                                [.{[2,\psep Det N]}
                                                    [.{[2,\psep the N]}
                                                        [.{[4,\psep N]}
                                                            [.{[4,\psep wolf]}
                                                            ]
                                                        ]
                                                    ]
                                                ]
                                                [.{[2,\psep Det AP N]}
                                                    [.{[2,\psep the AP N]}
                                                        [.{[3,\psep AP N]}
                                                            [.{[3,\psep Adj N]}
                                                                [.{[3,\psep bad N]}
                                                                ]
                                                                [.{[3,\psep big N]}
                                                                    [.{[5,\psep N]}
                                                                        [.{[5,\psep wolf]}
                                                                        ]
                                                                    ]
                                                                ]
                                                            ]
                                                            [.{[3,\psep Adj AP N]}
                                                                [.{[3,\psep bad AP N]}
                                                                ]
                                                                [.{[3,\psep big AP N]}
                                                                    [.{[4,\psep AP N]}
                                                                        [.{[4,\psep Adj N]}
                                                                            [.{[4,\psep bad N]}
                                                                                [.{[5,\psep N]}
                                                                                    [.{[5,\psep wolf]}
                                                                                        [.{[6,\psep]}
                                                                                        ]
                                                                                    ]
                                                                                ]
                                                                            ]
                                                                        ]
                                                                    ]
                                                                ]
                                                            ]
                                                        ]
                                                    ]
                                                ]
                                            ]
                                        ]
                                    ]
                                ]
                            ]
                        ]
                    ]
                ]
        \end{tikzpicture}
    \end{center}
    %
    As before we see that the recognizer repeats inferences that have already failed before, e.g.\ [1,\psep left NP] after [1,\psep left].
    A bigger problem is revealed once we look at the parse history after the recognizer correctly infers [1,\psep VP AdvP] from [1,\psep VP].
    %
    \begin{center}
        \footnotesize
        \begin{tikzpicture}[
            level 1+/.style = { level distance = 2em }
            ]
            \Tree
                [.{[1,\psep VP AdvP]}
                    [.{[1,\psep VP AdvP]}
                        [.{[1,\psep Vi AdvP]}
                            [.{[1, \psep left AdvP]}
                            ]
                        ]
                        [.{[1,\psep Vt NP AdvP]}
                            [.{[1,\psep left NP AdvP]}
                            ]
                            [.{[1,\psep met NP AdvP]}
                                [.{[2,\psep NP AdvP]}
                                    [.{[2,\psep PN AdvP]}
                                        [.{[2,\psep RRH AdvP]}
                                        ]
                                    ]
                                    [.{[2,\psep Det N AdvP]}
                                        [.{[2,\psep the N AdvP]}
                                            [.{[3,\psep N AdvP]}
                                                [.{[3,\psep wolf AdvP]}
                                                ]
                                            ]
                                        ]
                                        [.\node(link-top){\phantom{[]}};
                                        ]
                                    ]
                                ]
                            ]
                        ]
                    ]
                ]

            \begin{scope}[yshift=-22em]
                \Tree
                    [.\node(link-bottom){[3,\psep Det AP N AdvP]};
                        [.{[2,\psep the AP N AdvP]}
                            [.{[3,\psep AP N AdvP]}
                                [.{[3,\psep Adj N AdvP]}
                                    [.{[3,\psep bad N AdvP]}
                                    ]
                                    [.{[3,\psep big N AdvP]}
                                        [.{[4,\psep N AdvP]}
                                            [.{[4,\psep wolf AdvP]}
                                            ]
                                        ]
                                    ]
                                ]
                                [.{[3,\psep Adj AP N AdvP]}
                                    [.{[3,\psep bad AP N AdvP]}
                                    ]
                                    [.{[3,\psep big AP N AdvP]}
                                        [.{[4,\psep AP N AdvP]}
                                            [.{[4,\psep Adj N AdvP]}
                                                [.{[4,\psep bad N AdvP]}
                                                    [.{[5,\psep N AdvP]}
                                                        [.{[5,\psep wolf AdvP]}
                                                            [.{[6,\psep AdvP]}
                                                                [.{[6,\psep Adv]}
                                                                    [.{[6,\psep yesterday]}
                                                                        [.{[7,\psep]}
                                                                        ]
                                                                    ]
                                                                ]
                                                            ]
                                                        ]
                                                    ]
                                                ]
                                            ]
                                        ]
                                    ]
                                ]
                            ]
                        ]
                    ]
            \end{scope}

            \draw (link-top.north) .. controls +(310:14em) and +(55:6em) .. (link-bottom.north);    
        \end{tikzpicture}
        %
    \end{center}
    %
    This subtree of the parse history is virtually isomorphic to the one rooted in [1,\psep VP] --- the only difference is that the last branch expands the AdvP and finally reaches a goal item.
    What this means is that the recognizer does not reuse any of the information it collected on its first attempt to build the VP\@.
    It has to verify again that the verb is \emph{met}, and it builds the NP for \emph{big, bad wolf} from scratch even though it had already been correctly recognized in the [1,\psep VP] subtree.
\end{examplebox}
%
This shows that even for recognizers a good data structure is essential to
%
\begin{itemize}
    \item detect and avoid loops,
    \item skip inferences that have failed before,
    \item reuse successful inferences.
\end{itemize}
%
Once a useful data structure is in place, using it as a record of parse trees and thereby expanding the recognizer to a parser is just a minor step.

\section{Chart Parsing}

chart
agenda
dynamic programming/tabulation/memoization

\section{CKY}

\subsection{Intuition}

The Cock-Younger-Kasami algorithm --- usually abbreviated CYK or CKY --- is not only the best-known chart parsing algorithm, it is also the most popular parser in computational linguistics.
That's probably due to its high efficiency coupled with its conceptual simplicity.
The idea behind the CKY parser is indeed very simple: we read in the entire input string and then carry out all possible bottom-up reductions in parallel.
In a certain sense, the CKY parser behaves like a shift-reduce parser where \textsc{i}) we always apply shift until we have reached the end of the string, and \textsc{ii}) parse items can participate in multiple reduction steps.

The CKY parser is usually specified in the format of a chart parser.
Suppose we have the input string \emph{the anvil hit the duck on the head}, which should be analyzed with an extended version of our usual toy grammar.
%
\begin{center}
    \begin{tabular}{rrcl@{\hspace{3em}}rrcl}
        1)  & S   & \rewrite\ & NP VP               &
        \phantom{1}9)  & Det & \rewrite\ & a | the
        \\
        2)  & NP  & \rewrite\ & PN                  &
        10) & N   & \rewrite\ & car | truck | anvil | duck | hit
        \\
        3)  & NP  & \rewrite\ & Det N               &
        11) & PN  & \rewrite\ & Bugs | Daffy
        \\
        4)  & NP  & \rewrite\ & NP PP               &
        12) & P   & \rewrite\ & on
        \\
        5)  & PP  & \rewrite\ & P NP                &
        13)  & Vi  & \rewrite\ & fell over | duck
        \\
        6)  & VP  & \rewrite\ & Vi                  &
        14) & Vt  & \rewrite\ & hit
        \\
        7)  & VP  & \rewrite\ & Vt NP               &
        \\
        8)  & VP  & \rewrite\ & VP PP               &
    \end{tabular}
\end{center}
%
Our first step is to draw a matrix that has as many columns and rows as there are words in the input.
We will only use the fields above the diagonal, so the others are shaded out.
It will be convenient to refer to individual cells by their address $(i,j)$, where $i$ is the row number and $j$ the column number.
The idea is that each cell $(i,j)$ represents the substring of the input spanning from position $i$ to $j$.
%
\begin{center}
    \begin{tabular}{r | C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} |}
          & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
          \hline
        0 & & & & & & & & \\
        1 & \cellcolor{gray!25} & & & & & & & \\
        2 & \cellcolor{gray!25} & \cellcolor{gray!25} & & & & & & \\
        3 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & & & & & \\
        4 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & & & & \\
        5 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & & & \\
        6 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & & \\
        7 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \\
        \hline
          & the & anvil & hit & the & duck & on & the & head\\
    \end{tabular}
\end{center}
%
For each word $w_i$ in the input, we enter all possible parts of speech in the cell $(i,i+1)$.
%
\begin{center}
    \begin{tabular}{r | C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} |}
          & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
          \hline
        0 & Det & & & & & & & \\
        1 & \cellcolor{gray!25} & N & & & & & & \\
        2 & \cellcolor{gray!25} & \cellcolor{gray!25} & N,Vt & & & & & \\
        3 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & Det & & & & \\
        4 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & N,Vi & & & \\
        5 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & P & & \\
        6 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & Det & \\
        7 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & N\\
        \hline
          & the & anvil & hit & the & duck & on & the & head\\
    \end{tabular}
\end{center}
%
In order to fill in the remaining cells, we use the following algorithm: if cell $(i,j)$ contains category $B$ and cell $(j,k)$ contains category $C$ and $A \rewrite B C$ is a rewrite rule of the grammar, then enter $A$ in cell $(i,k)$.
%
\begin{center}
    \begin{tabular}{r | C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} |}
          & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
          \hline
        0 & Det & NP & & & S & & & S\\
        1 & \cellcolor{gray!25} & N & & & & & & \\
        2 & \cellcolor{gray!25} & \cellcolor{gray!25} & N,Vt & & VP & & & VP\\
        3 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & Det & NP & & & NP\\
        4 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & N,Vi & & & \\
        5 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & P & & PP\\
        6 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & Det & NP\\
        7 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & N\\
        \hline
          & the & anvil & hit & the & duck & on & the & head\\
    \end{tabular}
\end{center}
%
The input is well-formed iff the top-right cell contains the start category S\@.
In other words, there is a constituent labeled S that spans the entire input string.

With a chart like the one above, the CKY algorithm is just a recognizer as it is not always obvious how specific cell values were derived from others.
For instance, the cell $(3,5)$ contains the value NP, but there is no indication whether this NP consists of Det N or Det V\@.
Similarly, the VP in cell $(2,8)$ could be the result of combining either the V in $(2,3)$ with the NP in $(3,8)$ or the VP in $(2,5)$ with the PP in $(5,8)$.
These alternatives correspond to very different structures: one where the PP is an NP adjunct, and another one with the PP as a VP adjunct.

In order to turn CKY from a recognizer into a parser, we have to modify the cell values such that each category comes with backpointers that indicate which other categories were used in the reduction.
We can represent this pictorially by adding arrows to the chart.
%
\begin{center}
    \tikzexternaldisable
    \begin{tabular}{r | C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} |}
          & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
          \hline
        0 & \tikz[remember picture]{\node (01) {Det};} &
            \tikz[remember picture]{\node (02){NP};} & & &
            \tikz[remember picture]{\node (05){S};} & & &
            \tikz[remember picture]{\node (08-1){S};},%
            \tikz[remember picture]{\node (08-2){S};} \\
        1 & \cellcolor{gray!25} &
            \tikz[remember picture]{\node (12){N};} & & & & & & \\
        2 & \cellcolor{gray!25} & \cellcolor{gray!25} &
            \tikz[remember picture]{\node(23N){N};},%
            \tikz[remember picture]{\node(23V){Vt};} & &
            \tikz[remember picture]{\node(25){VP};} & & &
            \tikz[remember picture]{\node(28-1){VP};},%
            \tikz[remember picture]{\node(28-2){VP};}
            \\
        3 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} &
            \tikz[remember picture]{\node(34){Det};} &
            \tikz[remember picture]{\node(35){NP};} & & &
            \tikz[remember picture]{\node(38){NP};}\\
        4 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} &
            \tikz[remember picture]{\node(45N){N};},%
            \tikz[remember picture]{\node(45V){Vi};} & & & \\
        5 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} &
            \tikz[remember picture]{\node(56){P};} & &
            \tikz[remember picture]{\node(58){PP};}\\
        6 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} &
            \tikz[remember picture]{\node(67){Det};} &
            \tikz[remember picture]{\node(68){NP};}\\
        7 & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} & \cellcolor{gray!25} &
            \tikz[remember picture]{\node(78){N};}\\
        \hline
          & the & anvil & hit & the & duck & on & the & head\\
    \end{tabular}
    %
    \begin{tikzpicture}[remember picture,overlay]
        \foreach \Source/\Target in {%
                                     01/02,
                                     12/02,
                                     02/05,
                                     25/05,
                                     23V/25,
                                     35/25,
                                     34/35,
                                     45N/35,
                                     56/58,
                                     68/58,
                                     67/68,
                                     78/68,
                                     35/38,
                                     58/38,
                                     25/28-1,
                                     28-1/08-1,
                                     28-2/08-2%
                                    }
            \draw[<-,dashed,blue!55] (\Source) to (\Target);
            \draw[<-,dashed,blue!55,bend left] (58) to (28-1);
            \draw[<-,dashed,blue!55,bend right] (38) to (28-2);
            \draw[<-,dashed,blue!55,bend left=20] (23V) to (28-2);
            \draw[<-,dashed,blue!55,bend right=5] (02) to (08-1);
            \draw[<-,dashed,blue!55,bend right=8] (02) to (08-2.south west);

    \end{tikzpicture}
    \tikzexternalenable
\end{center}

\subsection{Alternative Data Structures}
You might have noticed right away that the chart with arrows between cells looks similar to a tree.
While it is not exactly a tree, the chart can indeed be viewed as a directed acyclic graph (DAG) like in Fig.~\ref{fig:Chart_DAG}.
A DAG is a set of nodes that are connected by branches that can only be followed in one direction (like the backpointers in the chart) and which are distributed in such a way that it is impossible to follow a branch out of a node and find a path back to that very same node.
In linguistic parlance, a directed acyclic graph would be a collection of one or more multi-dominance trees, each one of which can have multiple roots.
%
\begin{definition}[DAG]
    A \emph{graph} $G$ is a pair $\tuple{V,E}$ such that
    %
    \begin{itemize}
        \item $V$ is a set of \emph{vertices} (also called nodes),
        \item $E \subseteq V \times V$ of \emph{edges} (or arcs, branches) is a binary relation.
    \end{itemize}
    %
    A \emph{directed acyclic graph} (DAG) is a graph that satisfies the following axioms:
    %
    \begin{itemize}
        \item \textbf{Directedness}\\
            $E$ is asymmetric: $\tuple{x,y} \in E \rightarrow \tuple{y,x} \notin E$.
        \item \textbf{Acyclicity}\\
            The transitive closure of $E$ is irreflexive: $\tuple{x,x} \notin E^+$.
    \end{itemize}
\end{definition}
%
\begin{figure}
    \centering
    \begin{tikzpicture}[
        level 1+/.style = { level distance = 4em }
        ]
        \Tree
            [.\node(08-1){S};
                [.\node(02){NP};
                    [.\node(01){Det};
                        the
                    ]
                    [.\node(12){N};
                        anvil
                    ]
                ]
                [.\node(28-1){VP};
                    [.\node(25){VP};
                        [.\node(23){V};
                            hit
                        ]
                        [.\node(35){NP};
                            [.\node(34){Det};
                                the
                            ]
                            [.\node(45){N};
                                duck
                            ]
                        ]
                    ]
                    [.\node(58){PP};
                        [.\node(56){P};
                            on
                        ]
                        [.\node(68){NP};
                            [.\node(67){Det};
                                the
                            ]
                            [.\node(78){N};
                                head
                            ]
                        ]
                    ]
                ]
            ]

        \node (38) at ($(58) !.5! (23) + (1em,4em)$) {NP};
        \node (28-2) at ($(02) !.5! (28-1)$) {VP};
        \node (08-2) at ($(08-1) - (2.5em,0)$) {S};
        \node (08-3) at ($(08-1) + (2.5em,0)$) {S};

        \foreach \Source/\Target in {%
                                     58/38,
                                     35/38,
                                     23/28-2,
                                     38/28-2,
                                     02/08-2,
                                     28-2/08-2,
                                     02/08-3,
                                     25/08-3%
                                    }
            \draw (\Source.north) to (\Target.south);
    \end{tikzpicture}
\caption{Directed acyclic graph representation of a CKY chart with backpointers}
\label{fig:Chart_DAG}
\end{figure}
%
DAGs are used in a variety of parsing algorithms.
In fact, many of the most efficient parsers proposed since mid 80s use some kind of DAG-like data structure, e.g.\ the generalized LR parser, also known as the \emph{Tomita parser} \citep{Tomita86, Tomita87}, and generalized left corner parsing (\citealp{Nederhof93}; not to be confused with the notion of generalized left corner parser discussed in Sec.~\ref{sub:LeftCorner_Generalized} of Cha.~\ref{cha:LeftCorner}).
As you can see, the prefix \emph{generalized} is commonly used to denote parsers with some kind of graph-based component.

Yet another data structure keeps the basic notion of a chart but makes it easier to work with.
The standard CKY chart is somewhat odd in that all the cells below the diagonal are completely useless.
This is cumbersome for implementations, where it would be preferable if we could simply instantiate some kind of matrix and make full use of all its cells.
Fortunately enough this is easy to accomplish.

The charts above are displayed as 2-dimensional objects where a cell contains zero or more categories.
We can slightly modify this view by positing a third axis that lists all possible category symbols.
Then an entry like N,V in cell $(4,5)$ becomes a convenient shorthand for indicating that we have the value \emph{true} in two cells, $(4,5,\mathrm{N})$ and $(4,5,\mathrm{V})$.
Instead of a table, the chart is now a cube where the length of the $x$ and $y$-axes correspond to the length of the string and the $z$-axis is fixed by the number of categories in our grammar.
Like any cube, we can rotate this 3-dimensional chart so that we are facing another one of its $6$ sides.
Suppose then that we topple this cube over so that the $x$-axis stays the same but the $z$-axis becomes the $y$-axis (making the old $y$-axis the new $z$-axis).
When the cube is flattened back into a table at this point, we get a $2$-dimensional chart where the $x$-axis still records the end point of substrings, but the $y$-axis is now just the list of categories in our grammar.
In exchange, cells are no longer filled with categories but instead contain the starting index of substrings.
Consequently, a cell at position $(\mathrm{VP},8)$ with entry $2$ means that the substring spanning from $2$ to $8$ can be reduced to a VP\@.
Our example chart is repeated below using the new format.
%
\begin{center}
    \begin{tabular}{r | C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} C{3em} |}
          & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
          \hline
        S & & & & & 0 & & & 0\\
       NP & & 0 & & & 3 & & & 3\\
       VP & & & & & 2 & & & 2\\
       PP & & & & & & & & 5\\
      Det & 0& & & 3 & & & 6 & \\
       N  & & 1 & 2 & & 4 & & & 7 \\
       P  & & & & & & 5 & & \\
       PN & & & & & & & & \\
       Vi & & & & & 4 & & & \\
       Vt & & & 2 & & & & & \\
          \hline
          & the & anvil & hit & the & duck & on & the & head\\
    \end{tabular}
    %fixme: some numbers missing
\end{center}

This kind of table is much harder to decipher for humans, but it contains no wasted cells.
In addition, it behaves like any matrix over natural numbers and thus can be manipulated using well-understood (and efficiently implemented!) operations like matrix multiplication.

These examples are just the tip of the iceberg, the number of viable data structures is myriad.
That's why it is so important that we modularize parsers and study parsing schemata, control structures and data structures independently.
If we had taken a purely algorithmic approach, then each one of these parsers would look very different because a graph must be handled very differently from a table.
But as we will see next, the CKY parsing schema remains the same when we switch from charts to graphs or the other way round.

\subsection{Formal Specification}
The original CKY parsing algorithm combines two ideas: a mechanism for parallel bottom-up reduction and an efficient implementation of that idea via charts.
But since both are rolled into one algorithmic specification, it is needlessly hard to make out for the uninitiated what CKY parsing is really about.
As usual, the specification via a parsing schema is much more succinct and brings out the underlying logic more clearly.

In contrast to all the parsers we have seen so far, the CKY parser absolutely needs both indices in its parse items, so that the general form of parse items is $[i,\beta,j]$.
As another distinguishing property we have multiple axioms instead of just one.
For each rewrite rule $A \rewrite a$ such that $a = w_i$, there is a corresponding axiom $[i,A,i+1]$.
This emphasizes the parallel nature of the CKY-parser --- the parser is not at all incremental but rather operates on the whole string at once.
If desired, we could have no axiom at all and add an antecedent-less inference rule instead.
%
\begin{prooftree}
    \AxiomC{}
    \LeftLabel{\textbf{Shift}\qquad}
    \RightLabel{$A \rewrite a, a = w_i$}
    \UnaryInfC{$[i, A, i+1]$}
\end{prooftree}
%
The goal item of CKY is $[0,S,n]$, just like in our original parsing schema for bottom-up parsers.
Just from the goal and the axioms, then, we can already infer essential aspects of CKY parsing.

Only the inference rules remain to be specified, of which the CKY parser has only two.
%
\begin{prooftree}
    \AxiomC{$[i, B, j]$}
    \AxiomC{$[j, C, k]$}
    \LeftLabel{\textbf{Reduce}\qquad}
    \RightLabel{$A \rewrite B C \in R$}
    \BinaryInfC{$[i, A, k]$}
\end{prooftree}
%
It is instructive to contrast this version of the Reduce rule with the one used in a normal bottom-up parser.
There we infer from a single item $[i,\alpha \gamma \beta,j]$ the validity of a new item $[i,\alpha N \beta, j]$.
We may visualize this as adding some tree structure on top of a string of symbols.
The CKY parser, on the other hand, combines two subtrees into one tree.

Nevertheless the same effect can be obtained in a bottom-up parser.
Suppose that $B$ and $C$ in the pattern above are reduced from $[i,B_1,x] [x,B_2,j]$ and $[j,C_1,y] [y,C_2,k]$, respectively.
In the CKY parser we can go from these four items to $[i, A, k]$ via three reduction steps.
A breadth-first parser carries out a comparable reduction in the same number of steps.
%
\begin{center}
    \begin{tabular}{r|l}
        \textbf{parse item} & \textbf{inference rule}\\
        $[i,\alpha B_1 B_2 C_1 C_2 \beta, n]$ & already derived\\
        $[i,\alpha B C_1 C_2 \beta, n]$ & reduce\\
        $[i,\alpha B C \beta,n]$ & reduce\\
        $[i,\alpha A \beta,n]$ & reduce
    \end{tabular}
\end{center}
%
What makes the CKY parser special thus cannot be its inference rules.
Instead, it is the format of its parse items, which is ideally suited to memoization.

Suppose for the sake of argument that $B_1 B_2$ can also be reduced to $B'$ and $C_1 C_2$ to $C'$.
In the CKY parser, that is not much of an issue.
Two reductions for $B_1 B_2$ give us $B$ and $B'$, and two reductions for $C_1 C_2$ yield $C$ and $C'$.
If only $B C$ can be reduced further, we are guaranteed to arrive at $A$ in 5 steps.
The standard bottom-up parser takes longer.
As can be seen in Fig.~\ref{fig:Chart_BottomUpExtraSteps}, it may require 7 steps to successfully reduce $[i,\alpha B_1 B_2 C_1 C_2 \beta, n]$ to $[i,\alpha A \beta, n]$ (depending on which reductions are prioritized by the control structure).
This is purely due to the fact that reduction steps have to be repeated.
%
\begin{figure}
    \centering
    \begin{tikzpicture}
        \Tree
            [.{$[i,\alpha B_1 B_2 C_1 C_2 \beta,n]$}
                [.{$[i,\alpha B C_1 C_2 \beta, n]$}
                    [.{$[i,\alpha B C \beta,n ]$}
                        [.{$[i,\alpha A \beta, n]$}
                        ]
                    ]
                    [.{$[i,\alpha B C' \beta,n ]$}
                    ]
                ]
                [.{$[i,\alpha B' C_1 C_2 \beta, n]$}
                    [.{$[i,\alpha B' C \beta,n ]$}
                    ]
                    [.{$[i,\alpha B' C' \beta,n ]$}
                    ]
                ]
            ]
    \end{tikzpicture}
\caption{A standard bottom-up parse cannot efficiently reuse items and thus must take more steps}
\label{fig:Chart_BottomUpExtraSteps}
\end{figure}

\subsection{A Remark on Chomsky Normal Form}

Throughout the entire discussion so far, we have implicitly assumed that rewrite rules are in \emph{Chomsky Normal Form}.
%
\begin{definition}[Chomsky Normal Form]
    A context-free grammar is in Chomsky Normal Form (CNF) iff every rewrite rule is in one of two forms:
    %
    \begin{itemize}
        \item $A \rewrite B C$, where $A,B,C \in N$, or
        \item $A \rewrite a$, where $A\in N$ and $a \in T$.
    \end{itemize}
\end{definition}
%
As indicated by the term \emph{normal form}, every CFG can be brought into CNF\@.
%
\begin{exercise}
    Define an algorithm for transforming arbitrary CFGs into CNF\@.
\end{exercise}
%
\begin{exercise}
    Another common normal form for CFGs is \emph{Greibach Normal Form} (GNF), which only allows rewrite rules of the form $A \rewrite a \alpha$, where $\alpha \in N^*$.
    Can you define an algorithm that converts any arbitrary CFG into GNF as long as it does not generate the empty string?
    \emph{Hint:} You already know an algorithm that gets you halfway there.
\end{exercise}

It is often stated that the CKY-parser only works for grammars in CNF\@.
This statement is somewhat misleading.
The CKY algorithm, which combines the parsing schema from the previous section with a chart-based data structure, is indeed limited to CNF grammars.
This is so because the chart does not provide a good way of dealing with rewrite rules that have more than two symbols on their right-hand side.
The CKY parsing schema, on the other hand, can easily be generalized to arbitrary context-free grammars.
%
\begin{prooftree}
    \AxiomC{$[i, B_1, j_1]$}
    \AxiomC{$[j_1, B_2, j_2]\qquad \cdots$}
    \AxiomC{$[j_{n-1}, B_n, k]$}
    \LeftLabel{\textbf{Reduce}\qquad}
    \RightLabel{$A \rewrite B_1 B_2 \cdots B_n \in R$}
    \TrinaryInfC{$[i, A, k]$}
\end{prooftree}
%
Any known implementation is still much faster with CNF grammars, but the claim that CKY parsing only works for those is misleading.
% fixme: explain why it is faster

\subsection{Formalizing the Data Structure}
Agenda initialized as $\setof{[i, A, i+1] | A \rewrite a, a = w_i}$

\bibliographystyle{./bib/linquiry3}
\bibliography{./bib/universal,./bib/graf}

%fixme: more exercises
